{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-550043913c69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"freq\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rule\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "A Python implementation of the FP-growth algorithm.\n",
    "Basic usage of the module is very simple:\n",
    "    > from fp_growth import find_frequent_itemsets\n",
    "    > find_frequent_itemsets(transactions, minimum_support)\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "__author__ = \"Eric Naeseth <eric@naeseth.com>\"\n",
    "__copyright__ = \"Copyright Â© 2009 Eric Naeseth\"\n",
    "__license__ = \"MIT License\"\n",
    "\n",
    "\n",
    "def find_frequent_itemsets(transactions, minimum_support, include_support=False):\n",
    "    \"\"\"\n",
    "    Find frequent itemsets in the given transactions using FP-growth. This\n",
    "    function returns a generator instead of an eagerly-populated list of items.\n",
    "    The `transactions` parameter can be any iterable of iterables of items.\n",
    "    `minimum_support` should be an integer specifying the minimum number of\n",
    "    occurrences of an itemset for it to be accepted.\n",
    "    Each item must be hashable (i.e., it must be valid as a member of a\n",
    "    dictionary or a set).\n",
    "    If `include_support` is true, yield (itemset, support) pairs instead of\n",
    "    just the itemsets.\n",
    "    \"\"\"\n",
    "    items = defaultdict(lambda: 0)  # mapping from items to their supports\n",
    "\n",
    "    # if using support rate instead of support count\n",
    "    if 0 < minimum_support <= 1:\n",
    "        minimum_support = minimum_support * len(transactions)\n",
    "\n",
    "    # Load the passed-in transactions and count the support that individual\n",
    "    # items have.\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            items[item] += 1\n",
    "\n",
    "    # Remove infrequent items from the item support dictionary.\n",
    "    items = dict(\n",
    "        (item, support) for item, support in items.items() if support >= minimum_support\n",
    "    )\n",
    "\n",
    "    # Build our FP-tree. Before any transactions can be added to the tree, they\n",
    "    # must be stripped of infrequent items and their surviving items must be\n",
    "    # sorted in decreasing order of frequency.\n",
    "    def clean_transaction(transaction):\n",
    "        transaction = filter(lambda v: v in items, transaction)\n",
    "        transaction = sorted(transaction, key=lambda v: items[v], reverse=True)\n",
    "        return transaction\n",
    "\n",
    "    master = FPTree()\n",
    "    for transaction in list(map(clean_transaction, transactions)):\n",
    "        master.add(transaction)\n",
    "\n",
    "    def find_with_suffix(tree, suffix):\n",
    "        for item, nodes in tree.items():\n",
    "            support = sum(n.count for n in nodes)\n",
    "            if support >= minimum_support and item not in suffix:\n",
    "                # New winner!\n",
    "                found_set = [item] + suffix\n",
    "                yield (found_set, support) if include_support else found_set\n",
    "\n",
    "                # Build a conditional tree and recursively search for frequent\n",
    "                # itemsets within it.\n",
    "                cond_tree = conditional_tree_from_paths(tree.prefix_paths(item))\n",
    "                for s in find_with_suffix(cond_tree, found_set):\n",
    "                    yield s  # pass along the good news to our caller\n",
    "\n",
    "    # Search for frequent itemsets, and yield the results we find.\n",
    "    for itemset in find_with_suffix(master, []):\n",
    "        yield itemset\n",
    "\n",
    "\n",
    "class FPTree(object):\n",
    "    \"\"\"\n",
    "    An FP tree.\n",
    "    This object may only store transaction items that are hashable\n",
    "    (i.e., all items must be valid as dictionary keys or set members).\n",
    "    \"\"\"\n",
    "\n",
    "    Route = namedtuple(\"Route\", \"head tail\")\n",
    "\n",
    "    def __init__(self):\n",
    "        # The root node of the tree.\n",
    "        self._root = FPNode(self, None, None)\n",
    "\n",
    "        # A dictionary mapping items to the head and tail of a path of\n",
    "        # \"neighbors\" that will hit every node containing that item.\n",
    "        self._routes = {}\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        \"\"\"The root node of the tree.\"\"\"\n",
    "        return self._root\n",
    "\n",
    "    def add(self, transaction):\n",
    "        \"\"\"Add a transaction to the tree.\"\"\"\n",
    "        point = self._root\n",
    "\n",
    "        for item in transaction:\n",
    "            next_point = point.search(item)\n",
    "            if next_point:\n",
    "                # There is already a node in this tree for the current\n",
    "                # transaction item; reuse it.\n",
    "                next_point.increment()\n",
    "            else:\n",
    "                # Create a new point and add it as a child of the point we're\n",
    "                # currently looking at.\n",
    "                next_point = FPNode(self, item)\n",
    "                point.add(next_point)\n",
    "\n",
    "                # Update the route of nodes that contain this item to include\n",
    "                # our new node.\n",
    "                self._update_route(next_point)\n",
    "\n",
    "            point = next_point\n",
    "\n",
    "    def _update_route(self, point):\n",
    "        \"\"\"Add the given node to the route through all nodes for its item.\"\"\"\n",
    "        assert self is point.tree\n",
    "\n",
    "        try:\n",
    "            route = self._routes[point.item]\n",
    "            route[1].neighbor = point  # route[1] is the tail\n",
    "            self._routes[point.item] = self.Route(route[0], point)\n",
    "        except KeyError:\n",
    "            # First node for this item; start a new route.\n",
    "            self._routes[point.item] = self.Route(point, point)\n",
    "\n",
    "    def items(self):\n",
    "        \"\"\"\n",
    "        Generate one 2-tuples for each item represented in the tree. The first\n",
    "        element of the tuple is the item itself, and the second element is a\n",
    "        generator that will yield the nodes in the tree that belong to the item.\n",
    "        \"\"\"\n",
    "        for item in self._routes.keys():\n",
    "            yield (item, self.nodes(item))\n",
    "\n",
    "    def nodes(self, item):\n",
    "        \"\"\"\n",
    "        Generate the sequence of nodes that contain the given item.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            node = self._routes[item][0]\n",
    "        except KeyError:\n",
    "            return\n",
    "\n",
    "        while node:\n",
    "            yield node\n",
    "            node = node.neighbor\n",
    "\n",
    "    def prefix_paths(self, item):\n",
    "        \"\"\"Generate the prefix paths that end with the given item.\"\"\"\n",
    "\n",
    "        def collect_path(node):\n",
    "            path = []\n",
    "            while node and not node.root:\n",
    "                path.append(node)\n",
    "                node = node.parent\n",
    "            path.reverse()\n",
    "            return path\n",
    "\n",
    "        return (collect_path(node) for node in self.nodes(item))\n",
    "\n",
    "    def inspect(self):\n",
    "        print(\"Tree:\")\n",
    "        self.root.inspect(1)\n",
    "\n",
    "        print()\n",
    "        print(\"Routes:\")\n",
    "        for item, nodes in self.items():\n",
    "            print(\"  %r\" % item)\n",
    "            for node in nodes:\n",
    "                print(\"    %r\" % node)\n",
    "\n",
    "\n",
    "def conditional_tree_from_paths(paths):\n",
    "    \"\"\"Build a conditional FP-tree from the given prefix paths.\"\"\"\n",
    "    tree = FPTree()\n",
    "    condition_item = None\n",
    "    items = set()\n",
    "\n",
    "    # Import the nodes in the paths into the new tree. Only the counts of the\n",
    "    # leaf notes matter; the remaining counts will be reconstructed from the\n",
    "    # leaf counts.\n",
    "    for path in paths:\n",
    "        if condition_item is None:\n",
    "            condition_item = path[-1].item\n",
    "\n",
    "        point = tree.root\n",
    "        for node in path:\n",
    "            next_point = point.search(node.item)\n",
    "            if not next_point:\n",
    "                # Add a new node to the tree.\n",
    "                items.add(node.item)\n",
    "                count = node.count if node.item == condition_item else 0\n",
    "                next_point = FPNode(tree, node.item, count)\n",
    "                point.add(next_point)\n",
    "                tree._update_route(next_point)\n",
    "            point = next_point\n",
    "\n",
    "    assert condition_item is not None\n",
    "\n",
    "    # Calculate the counts of the non-leaf nodes.\n",
    "    for path in tree.prefix_paths(condition_item):\n",
    "        count = path[-1].count\n",
    "        for node in reversed(path[:-1]):\n",
    "            node._count += count\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "class FPNode(object):\n",
    "    \"\"\"A node in an FP tree.\"\"\"\n",
    "\n",
    "    def __init__(self, tree, item, count=1):\n",
    "        self._tree = tree\n",
    "        self._item = item\n",
    "        self._count = count\n",
    "        self._parent = None\n",
    "        self._children = {}\n",
    "        self._neighbor = None\n",
    "\n",
    "    def add(self, child):\n",
    "        \"\"\"Add the given FPNode `child` as a child of this node.\"\"\"\n",
    "\n",
    "        if not isinstance(child, FPNode):\n",
    "            raise TypeError(\"Can only add other FPNodes as children\")\n",
    "\n",
    "        if child.item not in self._children:\n",
    "            self._children[child.item] = child\n",
    "            child.parent = self\n",
    "\n",
    "    def search(self, item):\n",
    "        \"\"\"\n",
    "        Check whether this node contains a child node for the given item.\n",
    "        If so, that node is returned; otherwise, `None` is returned.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._children[item]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self._children\n",
    "\n",
    "    @property\n",
    "    def tree(self):\n",
    "        \"\"\"The tree in which this node appears.\"\"\"\n",
    "        return self._tree\n",
    "\n",
    "    @property\n",
    "    def item(self):\n",
    "        \"\"\"The item contained in this node.\"\"\"\n",
    "        return self._item\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        \"\"\"The count associated with this node's item.\"\"\"\n",
    "        return self._count\n",
    "\n",
    "    def increment(self):\n",
    "        \"\"\"Increment the count associated with this node's item.\"\"\"\n",
    "        if self._count is None:\n",
    "            raise ValueError(\"Root nodes have no associated count.\")\n",
    "        self._count += 1\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        \"\"\"True if this node is the root of a tree; false if otherwise.\"\"\"\n",
    "        return self._item is None and self._count is None\n",
    "\n",
    "    @property\n",
    "    def leaf(self):\n",
    "        \"\"\"True if this node is a leaf in the tree; false if otherwise.\"\"\"\n",
    "        return len(self._children) == 0\n",
    "\n",
    "    @property\n",
    "    def parent(self):\n",
    "        \"\"\"The node's parent\"\"\"\n",
    "        return self._parent\n",
    "\n",
    "    @parent.setter\n",
    "    def parent(self, value):\n",
    "        if value is not None and not isinstance(value, FPNode):\n",
    "            raise TypeError(\"A node must have an FPNode as a parent.\")\n",
    "        if value and value.tree is not self.tree:\n",
    "            raise ValueError(\"Cannot have a parent from another tree.\")\n",
    "        self._parent = value\n",
    "\n",
    "    @property\n",
    "    def neighbor(self):\n",
    "        \"\"\"\n",
    "        The node's neighbor; the one with the same value that is \"to the right\"\n",
    "        of it in the tree.\n",
    "        \"\"\"\n",
    "        return self._neighbor\n",
    "\n",
    "    @neighbor.setter\n",
    "    def neighbor(self, value):\n",
    "        if value is not None and not isinstance(value, FPNode):\n",
    "            raise TypeError(\"A node must have an FPNode as a neighbor.\")\n",
    "        if value and value.tree is not self.tree:\n",
    "            raise ValueError(\"Cannot have a neighbor from another tree.\")\n",
    "        self._neighbor = value\n",
    "\n",
    "    @property\n",
    "    def children(self):\n",
    "        \"\"\"The nodes that are children of this node.\"\"\"\n",
    "        return tuple(self._children.values())\n",
    "\n",
    "    def inspect(self, depth=0):\n",
    "        print((\"  \" * depth) + repr(self))\n",
    "        for child in self.children:\n",
    "            child.inspect(depth + 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.root:\n",
    "            return \"<%s (root)>\" % type(self).__name__\n",
    "        return \"<%s %r (%r)>\" % (type(self).__name__, self.item, self.count)\n",
    "\n",
    "\n",
    "def subs(l):\n",
    "    \"\"\"\n",
    "    Used for assoc_rule\n",
    "    \"\"\"\n",
    "    assert type(l) is list\n",
    "    if len(l) == 1:\n",
    "        return [l]\n",
    "    x = subs(l[1:])\n",
    "    return x + [[l[0]] + y for y in x]\n",
    "\n",
    "\n",
    "# Association rules\n",
    "def assoc_rule(freq, min_conf=0.6):\n",
    "    \"\"\"\n",
    "    This assoc_rule must input a dict for itemset -> support rate\n",
    "    And also can customize your minimum confidence\n",
    "    \"\"\"\n",
    "    assert type(freq) is dict\n",
    "    result = []\n",
    "    for item, sup in freq.items():\n",
    "        for subitem in subs(list(item)):\n",
    "            sb = [x for x in item if x not in subitem]\n",
    "            if sb == [] or subitem == []:\n",
    "                continue\n",
    "            if len(subitem) == 1 and (subitem[0][0] == \"in\" or subitem[0][0] == \"out\"):\n",
    "                continue\n",
    "            conf = sup / freq[tuple(subitem)]\n",
    "            if conf >= min_conf:\n",
    "                result.append({\"from\": subitem, \"to\": sb, \"sup\": sup, \"conf\": conf})\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from optparse import OptionParser\n",
    "    import csv\n",
    "\n",
    "    p = OptionParser(usage=\"%prog data_file\")\n",
    "    p.add_option(\n",
    "        \"-s\",\n",
    "        \"--minimum-support\",\n",
    "        dest=\"minsup\",\n",
    "        type=\"int\",\n",
    "        help=\"Minimum itemset support (default: 2)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-n\",\n",
    "        \"--numeric\",\n",
    "        dest=\"numeric\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Convert the values in datasets to numerals (default: false)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-c\",\n",
    "        \"--minimum-confidence\",\n",
    "        dest=\"minconf\",\n",
    "        type=\"float\",\n",
    "        help=\"Minimum rule confidence (default 0.6)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-f\",\n",
    "        \"--find\",\n",
    "        dest=\"find\",\n",
    "        type=\"str\",\n",
    "        help=\"Finding freq(frequency itemsets) or rule(association rules) (default: freq)\",\n",
    "    )\n",
    "    p.set_defaults(minsup=2)\n",
    "    p.set_defaults(numeric=False)\n",
    "    p.set_defaults(minconf=0.6)\n",
    "    p.set_defaults(find=\"freq\")\n",
    "    options, args = p.parse_args()\n",
    "\n",
    "    assert options.find == \"freq\" or options.find == \"rule\"\n",
    "\n",
    "    if len(args) < 1:\n",
    "        p.error(\"must provide the path to a CSV file to read\")\n",
    "\n",
    "    transactions = []\n",
    "    with open(args[0]) as database:\n",
    "        for row in csv.reader(database):\n",
    "            if options.numeric:\n",
    "                transaction = []\n",
    "                for item in row:\n",
    "                    transaction.append(int(item))\n",
    "                transactions.append(transaction)\n",
    "            else:\n",
    "                transactions.append(row)\n",
    "\n",
    "    result = []\n",
    "    res_for_rul = {}\n",
    "    for itemset, support in find_frequent_itemsets(transactions, options.minsup, True):\n",
    "        result.append((itemset, support))\n",
    "        res_for_rul[tuple(itemset)] = support\n",
    "\n",
    "    if options.find == \"freq\":\n",
    "        result = sorted(result, key=lambda i: i[0])\n",
    "        for itemset, support in result:\n",
    "            print(str(itemset) + \" \" + str(support))\n",
    "    if options.find == \"rule\":\n",
    "        rules = assoc_rule(res_for_rul, options.minconf)\n",
    "        for ru in rules:\n",
    "            print(str(ru[\"from\"]) + \" -> \" + str(ru[\"to\"]))\n",
    "            print(\"support = \" + str(ru[\"sup\"]) + \"confidence = \" + str(ru[\"conf\"]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-550043913c69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"freq\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rule\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "A Python implementation of the FP-growth algorithm.\n",
    "Basic usage of the module is very simple:\n",
    "    > from fp_growth import find_frequent_itemsets\n",
    "    > find_frequent_itemsets(transactions, minimum_support)\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "__author__ = \"Eric Naeseth <eric@naeseth.com>\"\n",
    "__copyright__ = \"Copyright Â© 2009 Eric Naeseth\"\n",
    "__license__ = \"MIT License\"\n",
    "\n",
    "\n",
    "def find_frequent_itemsets(transactions, minimum_support, include_support=False):\n",
    "    \"\"\"\n",
    "    Find frequent itemsets in the given transactions using FP-growth. This\n",
    "    function returns a generator instead of an eagerly-populated list of items.\n",
    "    The `transactions` parameter can be any iterable of iterables of items.\n",
    "    `minimum_support` should be an integer specifying the minimum number of\n",
    "    occurrences of an itemset for it to be accepted.\n",
    "    Each item must be hashable (i.e., it must be valid as a member of a\n",
    "    dictionary or a set).\n",
    "    If `include_support` is true, yield (itemset, support) pairs instead of\n",
    "    just the itemsets.\n",
    "    \"\"\"\n",
    "    items = defaultdict(lambda: 0)  # mapping from items to their supports\n",
    "\n",
    "    # if using support rate instead of support count\n",
    "    if 0 < minimum_support <= 1:\n",
    "        minimum_support = minimum_support * len(transactions)\n",
    "\n",
    "    # Load the passed-in transactions and count the support that individual\n",
    "    # items have.\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            items[item] += 1\n",
    "\n",
    "    # Remove infrequent items from the item support dictionary.\n",
    "    items = dict(\n",
    "        (item, support) for item, support in items.items() if support >= minimum_support\n",
    "    )\n",
    "\n",
    "    # Build our FP-tree. Before any transactions can be added to the tree, they\n",
    "    # must be stripped of infrequent items and their surviving items must be\n",
    "    # sorted in decreasing order of frequency.\n",
    "    def clean_transaction(transaction):\n",
    "        transaction = filter(lambda v: v in items, transaction)\n",
    "        transaction = sorted(transaction, key=lambda v: items[v], reverse=True)\n",
    "        return transaction\n",
    "\n",
    "    master = FPTree()\n",
    "    for transaction in list(map(clean_transaction, transactions)):\n",
    "        master.add(transaction)\n",
    "\n",
    "    def find_with_suffix(tree, suffix):\n",
    "        for item, nodes in tree.items():\n",
    "            support = sum(n.count for n in nodes)\n",
    "            if support >= minimum_support and item not in suffix:\n",
    "                # New winner!\n",
    "                found_set = [item] + suffix\n",
    "                yield (found_set, support) if include_support else found_set\n",
    "\n",
    "                # Build a conditional tree and recursively search for frequent\n",
    "                # itemsets within it.\n",
    "                cond_tree = conditional_tree_from_paths(tree.prefix_paths(item))\n",
    "                for s in find_with_suffix(cond_tree, found_set):\n",
    "                    yield s  # pass along the good news to our caller\n",
    "\n",
    "    # Search for frequent itemsets, and yield the results we find.\n",
    "    for itemset in find_with_suffix(master, []):\n",
    "        yield itemset\n",
    "\n",
    "\n",
    "class FPTree(object):\n",
    "    \"\"\"\n",
    "    An FP tree.\n",
    "    This object may only store transaction items that are hashable\n",
    "    (i.e., all items must be valid as dictionary keys or set members).\n",
    "    \"\"\"\n",
    "\n",
    "    Route = namedtuple(\"Route\", \"head tail\")\n",
    "\n",
    "    def __init__(self):\n",
    "        # The root node of the tree.\n",
    "        self._root = FPNode(self, None, None)\n",
    "\n",
    "        # A dictionary mapping items to the head and tail of a path of\n",
    "        # \"neighbors\" that will hit every node containing that item.\n",
    "        self._routes = {}\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        \"\"\"The root node of the tree.\"\"\"\n",
    "        return self._root\n",
    "\n",
    "    def add(self, transaction):\n",
    "        \"\"\"Add a transaction to the tree.\"\"\"\n",
    "        point = self._root\n",
    "\n",
    "        for item in transaction:\n",
    "            next_point = point.search(item)\n",
    "            if next_point:\n",
    "                # There is already a node in this tree for the current\n",
    "                # transaction item; reuse it.\n",
    "                next_point.increment()\n",
    "            else:\n",
    "                # Create a new point and add it as a child of the point we're\n",
    "                # currently looking at.\n",
    "                next_point = FPNode(self, item)\n",
    "                point.add(next_point)\n",
    "\n",
    "                # Update the route of nodes that contain this item to include\n",
    "                # our new node.\n",
    "                self._update_route(next_point)\n",
    "\n",
    "            point = next_point\n",
    "\n",
    "    def _update_route(self, point):\n",
    "        \"\"\"Add the given node to the route through all nodes for its item.\"\"\"\n",
    "        assert self is point.tree\n",
    "\n",
    "        try:\n",
    "            route = self._routes[point.item]\n",
    "            route[1].neighbor = point  # route[1] is the tail\n",
    "            self._routes[point.item] = self.Route(route[0], point)\n",
    "        except KeyError:\n",
    "            # First node for this item; start a new route.\n",
    "            self._routes[point.item] = self.Route(point, point)\n",
    "\n",
    "    def items(self):\n",
    "        \"\"\"\n",
    "        Generate one 2-tuples for each item represented in the tree. The first\n",
    "        element of the tuple is the item itself, and the second element is a\n",
    "        generator that will yield the nodes in the tree that belong to the item.\n",
    "        \"\"\"\n",
    "        for item in self._routes.keys():\n",
    "            yield (item, self.nodes(item))\n",
    "\n",
    "    def nodes(self, item):\n",
    "        \"\"\"\n",
    "        Generate the sequence of nodes that contain the given item.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            node = self._routes[item][0]\n",
    "        except KeyError:\n",
    "            return\n",
    "\n",
    "        while node:\n",
    "            yield node\n",
    "            node = node.neighbor\n",
    "\n",
    "    def prefix_paths(self, item):\n",
    "        \"\"\"Generate the prefix paths that end with the given item.\"\"\"\n",
    "\n",
    "        def collect_path(node):\n",
    "            path = []\n",
    "            while node and not node.root:\n",
    "                path.append(node)\n",
    "                node = node.parent\n",
    "            path.reverse()\n",
    "            return path\n",
    "\n",
    "        return (collect_path(node) for node in self.nodes(item))\n",
    "\n",
    "    def inspect(self):\n",
    "        print(\"Tree:\")\n",
    "        self.root.inspect(1)\n",
    "\n",
    "        print()\n",
    "        print(\"Routes:\")\n",
    "        for item, nodes in self.items():\n",
    "            print(\"  %r\" % item)\n",
    "            for node in nodes:\n",
    "                print(\"    %r\" % node)\n",
    "\n",
    "\n",
    "def conditional_tree_from_paths(paths):\n",
    "    \"\"\"Build a conditional FP-tree from the given prefix paths.\"\"\"\n",
    "    tree = FPTree()\n",
    "    condition_item = None\n",
    "    items = set()\n",
    "\n",
    "    # Import the nodes in the paths into the new tree. Only the counts of the\n",
    "    # leaf notes matter; the remaining counts will be reconstructed from the\n",
    "    # leaf counts.\n",
    "    for path in paths:\n",
    "        if condition_item is None:\n",
    "            condition_item = path[-1].item\n",
    "\n",
    "        point = tree.root\n",
    "        for node in path:\n",
    "            next_point = point.search(node.item)\n",
    "            if not next_point:\n",
    "                # Add a new node to the tree.\n",
    "                items.add(node.item)\n",
    "                count = node.count if node.item == condition_item else 0\n",
    "                next_point = FPNode(tree, node.item, count)\n",
    "                point.add(next_point)\n",
    "                tree._update_route(next_point)\n",
    "            point = next_point\n",
    "\n",
    "    assert condition_item is not None\n",
    "\n",
    "    # Calculate the counts of the non-leaf nodes.\n",
    "    for path in tree.prefix_paths(condition_item):\n",
    "        count = path[-1].count\n",
    "        for node in reversed(path[:-1]):\n",
    "            node._count += count\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "class FPNode(object):\n",
    "    \"\"\"A node in an FP tree.\"\"\"\n",
    "\n",
    "    def __init__(self, tree, item, count=1):\n",
    "        self._tree = tree\n",
    "        self._item = item\n",
    "        self._count = count\n",
    "        self._parent = None\n",
    "        self._children = {}\n",
    "        self._neighbor = None\n",
    "\n",
    "    def add(self, child):\n",
    "        \"\"\"Add the given FPNode `child` as a child of this node.\"\"\"\n",
    "\n",
    "        if not isinstance(child, FPNode):\n",
    "            raise TypeError(\"Can only add other FPNodes as children\")\n",
    "\n",
    "        if child.item not in self._children:\n",
    "            self._children[child.item] = child\n",
    "            child.parent = self\n",
    "\n",
    "    def search(self, item):\n",
    "        \"\"\"\n",
    "        Check whether this node contains a child node for the given item.\n",
    "        If so, that node is returned; otherwise, `None` is returned.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._children[item]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self._children\n",
    "\n",
    "    @property\n",
    "    def tree(self):\n",
    "        \"\"\"The tree in which this node appears.\"\"\"\n",
    "        return self._tree\n",
    "\n",
    "    @property\n",
    "    def item(self):\n",
    "        \"\"\"The item contained in this node.\"\"\"\n",
    "        return self._item\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        \"\"\"The count associated with this node's item.\"\"\"\n",
    "        return self._count\n",
    "\n",
    "    def increment(self):\n",
    "        \"\"\"Increment the count associated with this node's item.\"\"\"\n",
    "        if self._count is None:\n",
    "            raise ValueError(\"Root nodes have no associated count.\")\n",
    "        self._count += 1\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        \"\"\"True if this node is the root of a tree; false if otherwise.\"\"\"\n",
    "        return self._item is None and self._count is None\n",
    "\n",
    "    @property\n",
    "    def leaf(self):\n",
    "        \"\"\"True if this node is a leaf in the tree; false if otherwise.\"\"\"\n",
    "        return len(self._children) == 0\n",
    "\n",
    "    @property\n",
    "    def parent(self):\n",
    "        \"\"\"The node's parent\"\"\"\n",
    "        return self._parent\n",
    "\n",
    "    @parent.setter\n",
    "    def parent(self, value):\n",
    "        if value is not None and not isinstance(value, FPNode):\n",
    "            raise TypeError(\"A node must have an FPNode as a parent.\")\n",
    "        if value and value.tree is not self.tree:\n",
    "            raise ValueError(\"Cannot have a parent from another tree.\")\n",
    "        self._parent = value\n",
    "\n",
    "    @property\n",
    "    def neighbor(self):\n",
    "        \"\"\"\n",
    "        The node's neighbor; the one with the same value that is \"to the right\"\n",
    "        of it in the tree.\n",
    "        \"\"\"\n",
    "        return self._neighbor\n",
    "\n",
    "    @neighbor.setter\n",
    "    def neighbor(self, value):\n",
    "        if value is not None and not isinstance(value, FPNode):\n",
    "            raise TypeError(\"A node must have an FPNode as a neighbor.\")\n",
    "        if value and value.tree is not self.tree:\n",
    "            raise ValueError(\"Cannot have a neighbor from another tree.\")\n",
    "        self._neighbor = value\n",
    "\n",
    "    @property\n",
    "    def children(self):\n",
    "        \"\"\"The nodes that are children of this node.\"\"\"\n",
    "        return tuple(self._children.values())\n",
    "\n",
    "    def inspect(self, depth=0):\n",
    "        print((\"  \" * depth) + repr(self))\n",
    "        for child in self.children:\n",
    "            child.inspect(depth + 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.root:\n",
    "            return \"<%s (root)>\" % type(self).__name__\n",
    "        return \"<%s %r (%r)>\" % (type(self).__name__, self.item, self.count)\n",
    "\n",
    "\n",
    "def subs(l):\n",
    "    \"\"\"\n",
    "    Used for assoc_rule\n",
    "    \"\"\"\n",
    "    assert type(l) is list\n",
    "    if len(l) == 1:\n",
    "        return [l]\n",
    "    x = subs(l[1:])\n",
    "    return x + [[l[0]] + y for y in x]\n",
    "\n",
    "\n",
    "# Association rules\n",
    "def assoc_rule(freq, min_conf=0.6):\n",
    "    \"\"\"\n",
    "    This assoc_rule must input a dict for itemset -> support rate\n",
    "    And also can customize your minimum confidence\n",
    "    \"\"\"\n",
    "    assert type(freq) is dict\n",
    "    result = []\n",
    "    for item, sup in freq.items():\n",
    "        for subitem in subs(list(item)):\n",
    "            sb = [x for x in item if x not in subitem]\n",
    "            if sb == [] or subitem == []:\n",
    "                continue\n",
    "            if len(subitem) == 1 and (subitem[0][0] == \"in\" or subitem[0][0] == \"out\"):\n",
    "                continue\n",
    "            conf = sup / freq[tuple(subitem)]\n",
    "            if conf >= min_conf:\n",
    "                result.append({\"from\": subitem, \"to\": sb, \"sup\": sup, \"conf\": conf})\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from optparse import OptionParser\n",
    "    import csv\n",
    "\n",
    "    p = OptionParser(usage=\"%prog data_file\")\n",
    "    p.add_option(\n",
    "        \"-s\",\n",
    "        \"--minimum-support\",\n",
    "        dest=\"minsup\",\n",
    "        type=\"int\",\n",
    "        help=\"Minimum itemset support (default: 2)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-n\",\n",
    "        \"--numeric\",\n",
    "        dest=\"numeric\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Convert the values in datasets to numerals (default: false)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-c\",\n",
    "        \"--minimum-confidence\",\n",
    "        dest=\"minconf\",\n",
    "        type=\"float\",\n",
    "        help=\"Minimum rule confidence (default 0.6)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-f\",\n",
    "        \"--find\",\n",
    "        dest=\"find\",\n",
    "        type=\"str\",\n",
    "        help=\"Finding freq(frequency itemsets) or rule(association rules) (default: freq)\",\n",
    "    )\n",
    "    p.set_defaults(minsup=2)\n",
    "    p.set_defaults(numeric=False)\n",
    "    p.set_defaults(minconf=0.6)\n",
    "    p.set_defaults(find=\"freq\")\n",
    "    options, args = p.parse_args()\n",
    "\n",
    "    assert options.find == \"freq\" or options.find == \"rule\"\n",
    "\n",
    "    if len(args) < 1:\n",
    "        p.error(\"must provide the path to a CSV file to read\")\n",
    "\n",
    "    transactions = []\n",
    "    with open(args[0]) as database:\n",
    "        for row in csv.reader(database):\n",
    "            if options.numeric:\n",
    "                transaction = []\n",
    "                for item in row:\n",
    "                    transaction.append(int(item))\n",
    "                transactions.append(transaction)\n",
    "            else:\n",
    "                transactions.append(row)\n",
    "\n",
    "    result = []\n",
    "    res_for_rul = {}\n",
    "    for itemset, support in find_frequent_itemsets(transactions, options.minsup, True):\n",
    "        result.append((itemset, support))\n",
    "        res_for_rul[tuple(itemset)] = support\n",
    "\n",
    "    if options.find == \"freq\":\n",
    "        result = sorted(result, key=lambda i: i[0])\n",
    "        for itemset, support in result:\n",
    "            print(str(itemset) + \" \" + str(support))\n",
    "    if options.find == \"rule\":\n",
    "        rules = assoc_rule(res_for_rul, options.minconf)\n",
    "        for ru in rules:\n",
    "            print(str(ru[\"from\"]) + \" -> \" + str(ru[\"to\"]))\n",
    "            print(\"support = \" + str(ru[\"sup\"]) + \"confidence = \" + str(ru[\"conf\"]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>íìì¤í¬í</th>\n",
       "      <th>What genre of music do you most likely to listen to? /ê°ì¥ ì¢ìíë ìì ì¥ë¥´ë ë¬´ììëê¹?(Select 3 / 3ê°ì§ ì í)</th>\n",
       "      <th>What kind of fashion style do you like the most? / ê°ì¥ ì¢ìíë í¨ì ì¤íì¼ì ë¬´ììëê¹? (Select 3 / 3ê°ì§ ì í)</th>\n",
       "      <th>What kind of art performances do you like the most? / ê°ì¥ ì¢ìíë ê³µì°ì ë¬´ììëê¹?</th>\n",
       "      <th>What kind of art exhibitions do you like the most? / ê°ì¥ ì¢ìíë ì ìíë ë¬´ììëê¹?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019. 11. 7 ì¤í 6:03:37</td>\n",
       "      <td>Pop / Idol (í/ìì´ë), Ballad (ë°ë¼ë), Dance (ëì¤)</td>\n",
       "      <td>Workwear (ìí¬ì¨ì´ë¥µ) (constructed of sturdy fabric...</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Sculptures (ì¡°ê°)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019. 11. 7 ì¤í 6:04:10</td>\n",
       "      <td>Rap/ Hip Hop (ë©/íí©), R&amp;B / Soul (R&amp;B/ìì¸), Rock...</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Paintings (íí)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019. 11. 7 ì¤í 6:10:23</td>\n",
       "      <td>Pop (í), Ballad (ë°ë¼ë), R&amp;B / Soul (R&amp;B/ìì¸)</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Photography (ì¬ì§)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019. 11. 7 ì¤í 6:10:27</td>\n",
       "      <td>Pop (í), Rap/ Hip Hop (ë©/íí©), Indie (ì¸ë)</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Art Design (ëìì¸)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019. 11. 7 ì¤í 6:10:44</td>\n",
       "      <td>Idol (ìì´ë), Ballad (ë°ë¼ë), Dance (ëì¤)</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...</td>\n",
       "      <td>Musicals (ë®¤ì§ì»¬)</td>\n",
       "      <td>Art Design (ëìì¸)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    íìì¤í¬í  \\\n",
       "0  2019. 11. 7 ì¤í 6:03:37   \n",
       "1  2019. 11. 7 ì¤í 6:04:10   \n",
       "2  2019. 11. 7 ì¤í 6:10:23   \n",
       "3  2019. 11. 7 ì¤í 6:10:27   \n",
       "4  2019. 11. 7 ì¤í 6:10:44   \n",
       "\n",
       "  What genre of music do you most likely to listen to? /ê°ì¥ ì¢ìíë ìì ì¥ë¥´ë ë¬´ììëê¹?(Select 3 / 3ê°ì§ ì í)  \\\n",
       "0       Pop / Idol (í/ìì´ë), Ballad (ë°ë¼ë), Dance (ëì¤)                                               \n",
       "1  Rap/ Hip Hop (ë©/íí©), R&B / Soul (R&B/ìì¸), Rock...                                               \n",
       "2         Pop (í), Ballad (ë°ë¼ë), R&B / Soul (R&B/ìì¸)                                               \n",
       "3           Pop (í), Rap/ Hip Hop (ë©/íí©), Indie (ì¸ë)                                               \n",
       "4               Idol (ìì´ë), Ballad (ë°ë¼ë), Dance (ëì¤)                                               \n",
       "\n",
       "  What kind of fashion style do you like the most? / ê°ì¥ ì¢ìíë í¨ì ì¤íì¼ì ë¬´ììëê¹? (Select 3 / 3ê°ì§ ì í)  \\\n",
       "0  Workwear (ìí¬ì¨ì´ë¥µ) (constructed of sturdy fabric...                                              \n",
       "1  Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...                                              \n",
       "2  Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...                                              \n",
       "3  Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...                                              \n",
       "4  Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...                                              \n",
       "\n",
       "  What kind of art performances do you like the most? / ê°ì¥ ì¢ìíë ê³µì°ì ë¬´ììëê¹?  \\\n",
       "0                            Music Performances (ìì)                         \n",
       "1                            Music Performances (ìì)                         \n",
       "2                            Music Performances (ìì)                         \n",
       "3                            Music Performances (ìì)                         \n",
       "4                                     Musicals (ë®¤ì§ì»¬)                         \n",
       "\n",
       "  What kind of art exhibitions do you like the most? / ê°ì¥ ì¢ìíë ì ìíë ë¬´ììëê¹?  \n",
       "0                                    Sculptures (ì¡°ê°)                        \n",
       "1                                     Paintings (íí)                        \n",
       "2                                   Photography (ì¬ì§)                        \n",
       "3                                   Art Design (ëìì¸)                        \n",
       "4                                   Art Design (ëìì¸)                        "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_data = pd.read_csv('bi_data.csv')\n",
    "list_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>íìì¤í¬í</th>\n",
       "      <th>What genre of music do you most likely to listen to? /ê°ì¥ ì¢ìíë ìì ì¥ë¥´ë ë¬´ììëê¹?(Select 3 / 3ê°ì§ ì í)</th>\n",
       "      <th>What kind of fashion style do you like the most? / ê°ì¥ ì¢ìíë í¨ì ì¤íì¼ì ë¬´ììëê¹? (Select 3 / 3ê°ì§ ì í)</th>\n",
       "      <th>What kind of art performances do you like the most? / ê°ì¥ ì¢ìíë ê³µì°ì ë¬´ììëê¹?</th>\n",
       "      <th>What kind of art exhibitions do you like the most? / ê°ì¥ ì¢ìíë ì ìíë ë¬´ììëê¹?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019. 11. 7 ì¤í 6:03:37</td>\n",
       "      <td>Pop / Idol (í/ìì´ë), Ballad (ë°ë¼ë), Dance (ëì¤)</td>\n",
       "      <td>Workwear (ìí¬ì¨ì´ë¥µ) (constructed of sturdy fabric...</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Sculptures (ì¡°ê°)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019. 11. 7 ì¤í 6:04:10</td>\n",
       "      <td>Rap/ Hip Hop (ë©/íí©), R&amp;B / Soul (R&amp;B/ìì¸), Rock...</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Paintings (íí)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019. 11. 7 ì¤í 6:10:23</td>\n",
       "      <td>Pop (í), Ballad (ë°ë¼ë), R&amp;B / Soul (R&amp;B/ìì¸)</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Photography (ì¬ì§)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019. 11. 7 ì¤í 6:10:27</td>\n",
       "      <td>Pop (í), Rap/ Hip Hop (ë©/íí©), Indie (ì¸ë)</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Art Design (ëìì¸)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019. 11. 7 ì¤í 6:10:44</td>\n",
       "      <td>Idol (ìì´ë), Ballad (ë°ë¼ë), Dance (ëì¤)</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...</td>\n",
       "      <td>Musicals (ë®¤ì§ì»¬)</td>\n",
       "      <td>Art Design (ëìì¸)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>2019. 11. 15 ì¤í 4:07:30</td>\n",
       "      <td>Pop (í), Ballad (ë°ë¼ë), R&amp;B / Soul (R&amp;B/ìì¸)</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Street (ì¤í¸ë¦¿ë£©), Grunge (ê·¸ë°ì§ë£©)</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Paintings (íí)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>2019. 11. 15 ì¤í 4:36:31</td>\n",
       "      <td>Pop (í), R&amp;B / Soul (R&amp;B/ìì¸), Indie (ì¸ë)</td>\n",
       "      <td>Street (ì¤í¸ë¦¿ë£©), Vintage (ë¹í°ì§ë£©), Chic (ìí¬ë£©)</td>\n",
       "      <td>Musicals (ë®¤ì§ì»¬)</td>\n",
       "      <td>Paintings (íí)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2019. 11. 15 ì¤í 5:51:09</td>\n",
       "      <td>Pop (í), Idol (ìì´ë), R&amp;B / Soul (R&amp;B/ìì¸)</td>\n",
       "      <td>Workwear (ìí¬ì¨ì´ë¥µ) (constructed of sturdy fabric...</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Art Design (ëìì¸)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>2019. 11. 16 ì¤í 5:16:58</td>\n",
       "      <td>Pop (í), Rap/ Hip Hop (ë©/íí©), R&amp;B / Soul (R&amp;B/ìì¸)</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Street (ì¤í¸ë¦¿ë£©), Grunge (ê·¸ë°ì§ë£©)</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Paintings (íí)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2019. 11. 19 ì¤ì  11:42:31</td>\n",
       "      <td>Idol (ìì´ë), Dance (ëì¤), Indie (ì¸ë)</td>\n",
       "      <td>Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...</td>\n",
       "      <td>Music Performances (ìì)</td>\n",
       "      <td>Photography (ì¬ì§)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows Ã 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        íìì¤í¬í  \\\n",
       "0      2019. 11. 7 ì¤í 6:03:37   \n",
       "1      2019. 11. 7 ì¤í 6:04:10   \n",
       "2      2019. 11. 7 ì¤í 6:10:23   \n",
       "3      2019. 11. 7 ì¤í 6:10:27   \n",
       "4      2019. 11. 7 ì¤í 6:10:44   \n",
       "..                        ...   \n",
       "161   2019. 11. 15 ì¤í 4:07:30   \n",
       "162   2019. 11. 15 ì¤í 4:36:31   \n",
       "163   2019. 11. 15 ì¤í 5:51:09   \n",
       "164   2019. 11. 16 ì¤í 5:16:58   \n",
       "165  2019. 11. 19 ì¤ì  11:42:31   \n",
       "\n",
       "    What genre of music do you most likely to listen to? /ê°ì¥ ì¢ìíë ìì ì¥ë¥´ë ë¬´ììëê¹?(Select 3 / 3ê°ì§ ì í)  \\\n",
       "0         Pop / Idol (í/ìì´ë), Ballad (ë°ë¼ë), Dance (ëì¤)                                               \n",
       "1    Rap/ Hip Hop (ë©/íí©), R&B / Soul (R&B/ìì¸), Rock...                                               \n",
       "2           Pop (í), Ballad (ë°ë¼ë), R&B / Soul (R&B/ìì¸)                                               \n",
       "3             Pop (í), Rap/ Hip Hop (ë©/íí©), Indie (ì¸ë)                                               \n",
       "4                 Idol (ìì´ë), Ballad (ë°ë¼ë), Dance (ëì¤)                                               \n",
       "..                                                 ...                                               \n",
       "161         Pop (í), Ballad (ë°ë¼ë), R&B / Soul (R&B/ìì¸)                                               \n",
       "162           Pop (í), R&B / Soul (R&B/ìì¸), Indie (ì¸ë)                                               \n",
       "163           Pop (í), Idol (ìì´ë), R&B / Soul (R&B/ìì¸)                                               \n",
       "164  Pop (í), Rap/ Hip Hop (ë©/íí©), R&B / Soul (R&B/ìì¸)                                               \n",
       "165                 Idol (ìì´ë), Dance (ëì¤), Indie (ì¸ë)                                               \n",
       "\n",
       "    What kind of fashion style do you like the most? / ê°ì¥ ì¢ìíë í¨ì ì¤íì¼ì ë¬´ììëê¹? (Select 3 / 3ê°ì§ ì í)  \\\n",
       "0    Workwear (ìí¬ì¨ì´ë¥µ) (constructed of sturdy fabric...                                              \n",
       "1    Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...                                              \n",
       "2    Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...                                              \n",
       "3    Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...                                              \n",
       "4    Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...                                              \n",
       "..                                                 ...                                              \n",
       "161        Casual (ìºì£¼ì¼ë£©), Street (ì¤í¸ë¦¿ë£©), Grunge (ê·¸ë°ì§ë£©)                                              \n",
       "162          Street (ì¤í¸ë¦¿ë£©), Vintage (ë¹í°ì§ë£©), Chic (ìí¬ë£©)                                              \n",
       "163  Workwear (ìí¬ì¨ì´ë¥µ) (constructed of sturdy fabric...                                              \n",
       "164        Casual (ìºì£¼ì¼ë£©), Street (ì¤í¸ë¦¿ë£©), Grunge (ê·¸ë°ì§ë£©)                                              \n",
       "165  Casual (ìºì£¼ì¼ë£©), Workwear (ìí¬ì¨ì´ë¥µ) (constructed o...                                              \n",
       "\n",
       "    What kind of art performances do you like the most? / ê°ì¥ ì¢ìíë ê³µì°ì ë¬´ììëê¹?  \\\n",
       "0                              Music Performances (ìì)                         \n",
       "1                              Music Performances (ìì)                         \n",
       "2                              Music Performances (ìì)                         \n",
       "3                              Music Performances (ìì)                         \n",
       "4                                       Musicals (ë®¤ì§ì»¬)                         \n",
       "..                                                 ...                         \n",
       "161                            Music Performances (ìì)                         \n",
       "162                                     Musicals (ë®¤ì§ì»¬)                         \n",
       "163                            Music Performances (ìì)                         \n",
       "164                            Music Performances (ìì)                         \n",
       "165                            Music Performances (ìì)                         \n",
       "\n",
       "    What kind of art exhibitions do you like the most? / ê°ì¥ ì¢ìíë ì ìíë ë¬´ììëê¹?  \n",
       "0                                      Sculptures (ì¡°ê°)                        \n",
       "1                                       Paintings (íí)                        \n",
       "2                                     Photography (ì¬ì§)                        \n",
       "3                                     Art Design (ëìì¸)                        \n",
       "4                                     Art Design (ëìì¸)                        \n",
       "..                                                 ...                        \n",
       "161                                     Paintings (íí)                        \n",
       "162                                     Paintings (íí)                        \n",
       "163                                   Art Design (ëìì¸)                        \n",
       "164                                     Paintings (íí)                        \n",
       "165                                   Photography (ì¬ì§)                        \n",
       "\n",
       "[166 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_data = pd.read_csv('bi_data.csv')\n",
    "list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cd225bbf7136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[*]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "data = sc.textFile(\"bi_data.csv\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-70a91cb26643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminSupport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.csv\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-70a91cb26643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminSupport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.csv\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-023f160cc61a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "data = sc.textFile(\"bi_data.csv\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ea4b2c3ce596>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b078727c7183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-144ab1860cc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b078727c7183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4e91d34768ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0a5f448048e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/spark'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# add pyspark to sys.path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'py4j-*.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('C:/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-39fb9ae5f680>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminSupport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b078727c7183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# add pyspark to sys.path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'py4j-*.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
